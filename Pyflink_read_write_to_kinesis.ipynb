{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "640fe490-053a-4bc8-b4cb-8234646ab187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: apache-flink in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (1.19.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from apache-flink) (0.10.9.7)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from apache-flink) (2.9.0.post0)\n",
      "Requirement already satisfied: apache-beam<2.49.0,>=2.43.0 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from apache-flink) (2.48.0)\n",
      "Requirement already satisfied: cloudpickle>=2.2.0 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from apache-flink) (2.2.1)\n",
      "Requirement already satisfied: avro-python3!=1.9.2,>=1.8.1 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from apache-flink) (1.10.2)\n",
      "Requirement already satisfied: pytz>=2018.3 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from apache-flink) (2024.1)\n",
      "Requirement already satisfied: fastavro!=1.8.0,>=1.1.0 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from apache-flink) (1.9.4)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from apache-flink) (2.31.0)\n",
      "Requirement already satisfied: protobuf>=3.19.0 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from apache-flink) (4.23.4)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from apache-flink) (1.24.4)\n",
      "Requirement already satisfied: pandas>=1.3.0 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from apache-flink) (2.2.2)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from apache-flink) (11.0.0)\n",
      "Requirement already satisfied: httplib2>=0.19.0 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from apache-flink) (0.22.0)\n",
      "Requirement already satisfied: ruamel.yaml>=0.18.4 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from apache-flink) (0.18.6)\n",
      "Requirement already satisfied: apache-flink-libraries<1.19.1,>=1.19.0 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from apache-flink) (1.19.0)\n",
      "Requirement already satisfied: pemja==0.4.1 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from apache-flink) (0.4.1)\n",
      "Requirement already satisfied: find-libpython in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from pemja==0.4.1->apache-flink) (0.4.0)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from apache-beam<2.49.0,>=2.43.0->apache-flink) (1.7)\n",
      "Requirement already satisfied: orjson<4.0 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from apache-beam<2.49.0,>=2.43.0->apache-flink) (3.10.3)\n",
      "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from apache-beam<2.49.0,>=2.43.0->apache-flink) (0.3.1.1)\n",
      "Requirement already satisfied: fasteners<1.0,>=0.3 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from apache-beam<2.49.0,>=2.43.0->apache-flink) (0.19)\n",
      "Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from apache-beam<2.49.0,>=2.43.0->apache-flink) (1.63.0)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from apache-beam<2.49.0,>=2.43.0->apache-flink) (2.7.3)\n",
      "Requirement already satisfied: objsize<0.7.0,>=0.6.1 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from apache-beam<2.49.0,>=2.43.0->apache-flink) (0.6.1)\n",
      "Requirement already satisfied: pymongo<5.0.0,>=3.8.0 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from apache-beam<2.49.0,>=2.43.0->apache-flink) (4.7.2)\n",
      "Requirement already satisfied: proto-plus<2,>=1.7.1 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from apache-beam<2.49.0,>=2.43.0->apache-flink) (1.23.0)\n",
      "Requirement already satisfied: pydot<2,>=1.2.0 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from apache-beam<2.49.0,>=2.43.0->apache-flink) (1.4.2)\n",
      "Requirement already satisfied: regex>=2020.6.8 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from apache-beam<2.49.0,>=2.43.0->apache-flink) (2024.5.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.0 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from apache-beam<2.49.0,>=2.43.0->apache-flink) (4.11.0)\n",
      "Requirement already satisfied: zstandard<1,>=0.18.0 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from apache-beam<2.49.0,>=2.43.0->apache-flink) (0.22.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from httplib2>=0.19.0->apache-flink) (3.1.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from pandas>=1.3.0->apache-flink) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from python-dateutil<3,>=2.8.0->apache-flink) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from requests>=2.26.0->apache-flink) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from requests>=2.26.0->apache-flink) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from requests>=2.26.0->apache-flink) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from requests>=2.26.0->apache-flink) (2024.2.2)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from ruamel.yaml>=0.18.4->apache-flink) (0.2.8)\n",
      "Requirement already satisfied: docopt in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam<2.49.0,>=2.43.0->apache-flink) (0.6.2)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from pymongo<5.0.0,>=3.8.0->apache-beam<2.49.0,>=2.43.0->apache-flink) (2.6.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install apache-flink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eda5dc2b-b7e8-4736-81c3-7a64c350781e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Obtaining dependency information for boto3 from https://files.pythonhosted.org/packages/08/a9/34364af9b27ac3376f948a7ed4bfc1d4422fa22753d7a4b6f85cda549e42/boto3-1.34.107-py3-none-any.whl.metadata\n",
      "  Downloading boto3-1.34.107-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting botocore<1.35.0,>=1.34.107 (from boto3)\n",
      "  Obtaining dependency information for botocore<1.35.0,>=1.34.107 from https://files.pythonhosted.org/packages/89/59/105074130fa22bb0578920cfb25cc4cf4a2348b5ec04f113f7b5575f8528/botocore-1.34.107-py3-none-any.whl.metadata\n",
      "  Downloading botocore-1.34.107-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
      "  Obtaining dependency information for jmespath<2.0.0,>=0.7.1 from https://files.pythonhosted.org/packages/31/b4/b9b800c45527aadd64d5b442f9b932b00648617eb5d63d2c7a6587b7cafc/jmespath-1.0.1-py3-none-any.whl.metadata\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3)\n",
      "  Obtaining dependency information for s3transfer<0.11.0,>=0.10.0 from https://files.pythonhosted.org/packages/83/37/395cdb6ee92925fa211e55d8f07b9f93cf93f60d7d4ce5e66fd73f1ea986/s3transfer-0.10.1-py3-none-any.whl.metadata\n",
      "  Downloading s3transfer-0.10.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from botocore<1.35.0,>=1.34.107->boto3) (2.9.0.post0)\n",
      "Collecting urllib3<1.27,>=1.25.4 (from botocore<1.35.0,>=1.34.107->boto3)\n",
      "  Obtaining dependency information for urllib3<1.27,>=1.25.4 from https://files.pythonhosted.org/packages/b0/53/aa91e163dcfd1e5b82d8a890ecf13314e3e149c05270cc644581f77f17fd/urllib3-1.26.18-py2.py3-none-any.whl.metadata\n",
      "  Using cached urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/rahulsrivastav/Documents/Spark/venv/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.107->boto3) (1.16.0)\n",
      "Downloading boto3-1.34.107-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.34.107-py3-none-any.whl (12.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.1\n",
      "    Uninstalling urllib3-2.2.1:\n",
      "      Successfully uninstalled urllib3-2.2.1\n",
      "Successfully installed boto3-1.34.107 botocore-1.34.107 jmespath-1.0.1 s3transfer-0.10.1 urllib3-1.26.18\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd16cacc-924d-45c8-a081-73f304f6577a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "\n",
    "def create_kinesis_stream(stream_name, shard_count):\n",
    "    try:\n",
    "        # Initialize the Kinesis client\n",
    "        kinesis_client = boto3.client(\n",
    "            'kinesis', \n",
    "            region_name='ap-south-1',\n",
    "            endpoint_url='http://localhost:4566',\n",
    "            aws_access_key_id='dummy',  \n",
    "            aws_secret_access_key='dummy'\n",
    "        )\n",
    "\n",
    "        # Create the Kinesis stream\n",
    "        response = kinesis_client.create_stream(\n",
    "            StreamName=stream_name,\n",
    "            ShardCount=shard_count\n",
    "        )\n",
    "\n",
    "        # Check for successful response\n",
    "        if response['ResponseMetadata']['HTTPStatusCode'] == 200:\n",
    "            print(f\"Kinesis stream '{stream_name}' created with {shard_count} shard(s)\")\n",
    "        else:\n",
    "            print(\"Failed to create Kinesis stream\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "def delete_kinesis_stream(stream_name):\n",
    "    try:\n",
    "        # Initialize the Kinesis client\n",
    "        kinesis_client = boto3.client('kinesis')\n",
    "\n",
    "        # Delete the Kinesis stream\n",
    "        response = kinesis_client.delete_stream(\n",
    "            StreamName=stream_name\n",
    "        )\n",
    "\n",
    "        # Check for successful response\n",
    "        if response['ResponseMetadata']['HTTPStatusCode'] == 200:\n",
    "            print(f\"Kinesis stream '{stream_name}' deleted successfully\")\n",
    "        else:\n",
    "            print(\"Failed to delete Kinesis stream\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2fad323-b28e-4f0c-a926-e4487ae40053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kinesis stream 'input-stream' created with 1 shard(s)\n"
     ]
    }
   ],
   "source": [
    "create_kinesis_stream(\"input-stream\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f4c97e0-46fc-40b2-beb1-47cfa5ca436b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event_time': '2024-05-17T15:40:21.397776', 'ticker': 'AAPL', 'price': 44.95}\n",
      "{'event_time': '2024-05-17T15:40:21.468428', 'ticker': 'AMZN', 'price': 82.2}\n",
      "{'event_time': '2024-05-17T15:40:21.504897', 'ticker': 'INTC', 'price': 78.13}\n",
      "{'event_time': '2024-05-17T15:40:21.539296', 'ticker': 'AAPL', 'price': 16.74}\n",
      "{'event_time': '2024-05-17T15:40:21.660122', 'ticker': 'MSFT', 'price': 20.48}\n",
      "{'event_time': '2024-05-17T15:40:21.681035', 'ticker': 'AAPL', 'price': 4.53}\n",
      "{'event_time': '2024-05-17T15:40:21.700507', 'ticker': 'AAPL', 'price': 69.56}\n",
      "{'event_time': '2024-05-17T15:40:21.718740', 'ticker': 'AMZN', 'price': 92.3}\n",
      "{'event_time': '2024-05-17T15:40:21.742031', 'ticker': 'AAPL', 'price': 18.17}\n",
      "{'event_time': '2024-05-17T15:40:21.758645', 'ticker': 'TBV', 'price': 0.83}\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import boto3\n",
    "\n",
    "STREAM_NAME = \"input-stream\"\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    return {\n",
    "        'event_time': datetime.datetime.now().isoformat(),\n",
    "        'ticker': random.choice(['AAPL', 'AMZN', 'MSFT', 'INTC', 'TBV']),\n",
    "        'price': round(random.random() * 100, 2)\n",
    "    }\n",
    "\n",
    "\n",
    "def generate(stream_name, kinesis_client, num_samples):\n",
    "    for _ in range(num_samples):\n",
    "        data = get_data()\n",
    "        print(data)\n",
    "        kinesis_client.put_record(\n",
    "            StreamName=stream_name,\n",
    "            Data=json.dumps(data),\n",
    "            PartitionKey=\"partitionkey\")\n",
    "if __name__ == '__main__':\n",
    "    num_samples = 10  # Change this to the desired number of samples\n",
    "    client = boto3.client(\n",
    "            'kinesis', \n",
    "            region_name='ap-south-1',\n",
    "            endpoint_url='http://localhost:4566',\n",
    "            aws_access_key_id='dummy',  \n",
    "            aws_secret_access_key='dummy'\n",
    "        )\n",
    "    \n",
    "    generate(STREAM_NAME, client, num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d75bb3b3-27f3-4f73-9a96-7a32f1f63761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kinesis_part_1',\n",
       " '.ipynb_checkpoints',\n",
       " 'flink-sql-connector-kinesis-1.16.1.jar']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7473df3-fa1d-4a73-b4a5-27ba9a7977ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d6b81410-7cd2-4c10-9fab-ac220fba1e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Assuming LocalStack is running on localhost with default port 4566\n",
    "endpoint_url = 'http://localhost:4566'\n",
    "region_name = 'ap-south-1'  # This can be any region for LocalStack\n",
    "\n",
    "# Create a Kinesis client\n",
    "kinesis_client = boto3.client('kinesis', endpoint_url=endpoint_url, region_name=region_name)\n",
    "\n",
    "# List all Kinesis streams\n",
    "response = kinesis_client.list_streams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fbcb9a56-eb9c-441f-8a40-ae976349d6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Kinesis streams:\n",
      "input-stream\n"
     ]
    }
   ],
   "source": [
    "# Extract and print stream names\n",
    "if 'StreamNames' in response:\n",
    "    print(\"List of Kinesis streams:\")\n",
    "    for stream_name in response['StreamNames']:\n",
    "        print(stream_name)\n",
    "else:\n",
    "    print(\"No Kinesis streams found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a070a7-16a2-4722-88c7-00def8e45c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created source tables \n",
      "Executing query: SELECT * FROM input_table\n",
      "<Row('AAPL', 44.95, datetime.datetime(2024, 5, 17, 15, 40, 21, 397000))>\n",
      "<Row('AMZN', 82.2, datetime.datetime(2024, 5, 17, 15, 40, 21, 468000))>\n",
      "<Row('INTC', 78.13, datetime.datetime(2024, 5, 17, 15, 40, 21, 504000))>\n",
      "<Row('AAPL', 16.74, datetime.datetime(2024, 5, 17, 15, 40, 21, 539000))>\n",
      "<Row('MSFT', 20.48, datetime.datetime(2024, 5, 17, 15, 40, 21, 660000))>\n",
      "<Row('AAPL', 4.53, datetime.datetime(2024, 5, 17, 15, 40, 21, 681000))>\n",
      "<Row('AAPL', 69.56, datetime.datetime(2024, 5, 17, 15, 40, 21, 700000))>\n",
      "<Row('AMZN', 92.3, datetime.datetime(2024, 5, 17, 15, 40, 21, 718000))>\n",
      "<Row('AAPL', 18.17, datetime.datetime(2024, 5, 17, 15, 40, 21, 742000))>\n",
      "<Row('TBV', 0.83, datetime.datetime(2024, 5, 17, 15, 40, 21, 758000))>\n",
      "<Row('TBV1', 36.14, datetime.datetime(2024, 5, 17, 16, 33, 39, 8000))>\n",
      "<Row('INTC1', 98.19, datetime.datetime(2024, 5, 17, 16, 33, 39, 40000))>\n",
      "<Row('AAPL1', 74.61, datetime.datetime(2024, 5, 17, 16, 33, 39, 50000))>\n",
      "<Row('AMZN1', 11.5, datetime.datetime(2024, 5, 17, 16, 33, 39, 59000))>\n",
      "<Row('TBV1', 69.86, datetime.datetime(2024, 5, 17, 16, 33, 39, 68000))>\n",
      "<Row('INTC1', 16.56, datetime.datetime(2024, 5, 17, 16, 33, 39, 76000))>\n",
      "<Row('AAPL1', 78.96, datetime.datetime(2024, 5, 17, 16, 33, 39, 84000))>\n",
      "<Row('AAPL1', 11.23, datetime.datetime(2024, 5, 17, 16, 33, 39, 94000))>\n",
      "<Row('TBV1', 82.43, datetime.datetime(2024, 5, 17, 16, 33, 39, 102000))>\n",
      "<Row('AAPL1', 80.09, datetime.datetime(2024, 5, 17, 16, 33, 39, 111000))>\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyflink.table import EnvironmentSettings, TableEnvironment\n",
    "import os\n",
    "\n",
    "# 1. Create a Table Environment\n",
    "env_settings = EnvironmentSettings.in_streaming_mode()\n",
    "table_env = TableEnvironment.create(env_settings)\n",
    "\n",
    "# Define configuration properties for the Flink job\n",
    "props = [\n",
    "    {\n",
    "        \"PropertyGroupId\": \"kinesis.analytics.flink.run.options\",\n",
    "        \"PropertyMap\": {\n",
    "            \"python\": \"GettingStarted/getting-started.py\",  # Python script to be executed\n",
    "            \"jarfile\": \"flink-sql-connector-kinesis-1.16.1.jar\"  # Flink connector JAR file\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"PropertyGroupId\": \"consumer.config.0\",\n",
    "        \"PropertyMap\": {\n",
    "            \"input.stream.name\": \"input-stream\",        # Name of the Kinesis Data Stream\n",
    "            \"flink.stream.initpos\": \"TRIM_HORIZON\",     # Initial position to start reading the stream\n",
    "            \"aws.region\": \"ap-south-1\"                   # AWS region where the stream is located\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Function to extract property map from the configuration properties\n",
    "def property_map(props, property_group_id):\n",
    "    for prop in props:\n",
    "        if prop[\"PropertyGroupId\"] == property_group_id:\n",
    "            return prop[\"PropertyMap\"]\n",
    "\n",
    "# Function to create a source table definition for Kinesis stream\n",
    "def create_source_table(table_name, stream_name, region, stream_initpos):\n",
    "    return f\"\"\" \n",
    "    CREATE TABLE {table_name} (\n",
    "        ticker VARCHAR(6),\n",
    "        price DOUBLE,\n",
    "        event_time TIMESTAMP(3),\n",
    "        WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND\n",
    "    )\n",
    "    PARTITIONED BY (ticker)\n",
    "    WITH (\n",
    "        'connector' = 'kinesis',\n",
    "        'stream' = '{stream_name}',\n",
    "        'aws.region' = '{region}',\n",
    "        'aws.endpoint' = 'http://localhost:4566',\n",
    "        'aws.credentials.basic.accesskeyid' = 'dummy',\n",
    "        'aws.credentials.basic.secretkey' = 'dummy',\n",
    "        'scan.stream.initpos' = '{stream_initpos}',\n",
    "        'format' = 'json',\n",
    "        'json.timestamp-format.standard' = 'ISO-8601'\n",
    "    ) \n",
    "    \"\"\"\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Define keys for accessing properties\n",
    "    input_property_group_key = \"consumer.config.0\"\n",
    "    input_stream_key = \"input.stream.name\"\n",
    "    input_region_key = \"aws.region\"\n",
    "    input_starting_position_key = \"flink.stream.initpos\"\n",
    "\n",
    "    # Table name for the source data\n",
    "    input_table_name = \"input_table\"\n",
    "\n",
    "    # Get application properties from the props list\n",
    "    input_property_map = property_map(props, input_property_group_key)\n",
    "    input_stream = input_property_map[input_stream_key]\n",
    "    input_region = input_property_map[input_region_key]\n",
    "    stream_initpos = input_property_map[input_starting_position_key]\n",
    "\n",
    "    # Check if running in a local environment and set pipeline.jars configuration\n",
    "    CURRENT_DIR = os.getcwd()  # Get the current working directory\n",
    "    table_env.get_config().get_configuration().set_string(\n",
    "        \"pipeline.jars\",\n",
    "        \"file:///\" + CURRENT_DIR + \"/flink-sql-connector-kinesis-1.16.1.jar\",\n",
    "    )\n",
    "\n",
    "    # 2. Create a source table from a Kinesis Data Stream\n",
    "    table_env.execute_sql(\n",
    "        create_source_table(input_table_name, input_stream, input_region, stream_initpos)\n",
    "    )\n",
    "\n",
    "    print(\"Successfully created source tables \")\n",
    "\n",
    "    # 3. Read data from the source table and print it to the console\n",
    "    query = f\"SELECT * FROM {input_table_name}\"\n",
    "    print(f\"Executing query: {query}\")\n",
    "    \n",
    "    table_result = table_env.execute_sql(query)\n",
    "    \n",
    "    # Printing results\n",
    "    with table_result.collect() as results:\n",
    "        for result in results:\n",
    "            print(result)\n",
    "\n",
    "    if os.environ.get(\"IS_LOCAL\"):\n",
    "        # In a local environment, execute the job synchronously\n",
    "        table_env.execute(\"Getting Started Job\")\n",
    "    else:\n",
    "        # In a remote environment, execute the job asynchronously and print the job status\n",
    "        job_client = table_env.execute_async(\"Getting Started Job\")\n",
    "        print(job_client.get_job_status())\n",
    "\n",
    "\n",
    "# Entry point for the script\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c12684d-b4dc-4dc3-a9fa-b0343e57d9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f924e03-f738-4b11-b346-f1adf09b3348",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"dummy\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"dummy\"\n",
    "os.environ[\"AWS_REGION\"] = \"ap-south-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa8aeff-160d-4b52-bf66-e59d4e118d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "hudi_output_path = 's3a://dXXX/tmp1/'\n",
    "\n",
    "hudi_sink = f\"\"\"\n",
    "CREATE TABLE customers(\n",
    "    uuid VARCHAR(36) PRIMARY KEY NOT ENFORCED,\n",
    "    first_name VARCHAR(50),\n",
    "    city VARCHAR(50),\n",
    "    state VARCHAR(50)\n",
    ")\n",
    "PARTITIONED BY (`state`)\n",
    "WITH (\n",
    "    'connector' = 'hudi',\n",
    "    'path' = '{hudi_output_path}' ,\n",
    "    'table.type' = 'MERGE_ON_READ' \n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL to create the Hudi table\n",
    "table_env.execute_sql(hudi_sink)\n",
    "\n",
    "# Define the data to be inserted into the Hudi table\n",
    "table_env.execute_sql(\"\"\"\n",
    "    INSERT INTO customers\n",
    "        SELECT * FROM source_table\n",
    "\"\"\").wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3158e58b-5d1b-4c81-9b0c-202299332cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
