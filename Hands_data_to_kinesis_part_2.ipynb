{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2201d419-7749-441c-9355-9e3dc94bd82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "210f0bc4-661b-44c0-8e3a-37d85c394680",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"dummy\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"dummy\"\n",
    "os.environ[\"AWS_ENDPOINT_URL\"] = \"http://localhost:4566\"\n",
    "os.environ[\"AWS_REGION\"] = \"ap-south-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb1b806f-da4a-4128-9407-f506284cbe9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created source tables \n",
      "Executing query: SELECT * FROM input_table\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyflink.table import EnvironmentSettings, TableEnvironment\n",
    "import os\n",
    "\n",
    "# 1. Create a Table Environment\n",
    "env_settings = EnvironmentSettings.in_streaming_mode()\n",
    "table_env = TableEnvironment.create(env_settings)\n",
    "\n",
    "# Define configuration properties for the Flink job\n",
    "props = [\n",
    "    {\n",
    "        \"PropertyGroupId\": \"kinesis.analytics.flink.run.options\",\n",
    "        \"PropertyMap\": {\n",
    "            \"python\": \"GettingStarted/getting-started.py\",  # Python script to be executed\n",
    "            \"jarfile\": \"flink-sql-connector-kinesis-1.16.1.jar\"  # Flink connector JAR file\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"PropertyGroupId\": \"consumer.config.0\",\n",
    "        \"PropertyMap\": {\n",
    "            \"input.stream.name\": \"input-stream\",        # Name of the Kinesis Data Stream\n",
    "            \"flink.stream.initpos\": \"TRIM_HORIZON\",     # Initial position to start reading the stream\n",
    "            \"aws.region\": \"ap-south-1\"                   # AWS region where the stream is located\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Function to extract property map from the configuration properties\n",
    "def property_map(props, property_group_id):\n",
    "    for prop in props:\n",
    "        if prop[\"PropertyGroupId\"] == property_group_id:\n",
    "            return prop[\"PropertyMap\"]\n",
    "\n",
    "# Function to create a source table definition for Kinesis stream\n",
    "# TODO windowing on this create table statement to store the states\n",
    "def create_source_table(table_name, stream_name, region, stream_initpos):\n",
    "    return f\"\"\" \n",
    "    CREATE TABLE {table_name} (\n",
    "        event_uuid VARCHAR,\n",
    "        event_name VARCHAR,\n",
    "        event_data VARCHAR,\n",
    "        event_type VARCHAR,\n",
    "        event_timestamp INTEGER,\n",
    "        shard_id INTEGER,\n",
    "        event_source VARCHAR,\n",
    "        event_version VARCHAR\n",
    "    )\n",
    "    PARTITIONED BY (event_name)\n",
    "    WITH (\n",
    "        'connector' = 'kinesis',\n",
    "        'stream' = '{stream_name}',\n",
    "        'aws.region' = '{region}',\n",
    "        'aws.endpoint' = 'http://localhost:4566',\n",
    "        'aws.credentials.basic.accesskeyid' = 'dummy',\n",
    "        'aws.credentials.basic.secretkey' = 'dummy',\n",
    "        'scan.stream.initpos' = '{stream_initpos}',\n",
    "        'format' = 'json',\n",
    "        'json.timestamp-format.standard' = 'ISO-8601'\n",
    "    ) \n",
    "    \"\"\"\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Define keys for accessing properties\n",
    "    input_property_group_key = \"consumer.config.0\"\n",
    "    input_stream_key = \"input.stream.name\"\n",
    "    input_region_key = \"aws.region\"\n",
    "    input_starting_position_key = \"flink.stream.initpos\"\n",
    "\n",
    "    # Table name for the source data\n",
    "    input_table_name = \"input_table\"\n",
    "\n",
    "    # Get application properties from the props list\n",
    "    input_property_map = property_map(props, input_property_group_key)\n",
    "    input_stream = input_property_map[input_stream_key]\n",
    "    input_region = input_property_map[input_region_key]\n",
    "    stream_initpos = input_property_map[input_starting_position_key]\n",
    "\n",
    "    # Check if running in a local environment and set pipeline.jars configuration\n",
    "    CURRENT_DIR = os.getcwd()  # Get the current working directory\n",
    "    table_env.get_config().get_configuration().set_string(\n",
    "        \"pipeline.jars\",\n",
    "        \"file:///\" + CURRENT_DIR + \"/flink-sql-connector-kinesis-1.16.1.jar\",\n",
    "    )\n",
    "\n",
    "    # 2. Create a source table from a Kinesis Data Stream\n",
    "    table_env.execute_sql(\n",
    "        create_source_table(input_table_name, input_stream, input_region, stream_initpos)\n",
    "    )\n",
    "\n",
    "    print(\"Successfully created source tables \")\n",
    "\n",
    "    # 3. Read data from the source table and print it to the console\n",
    "    query = f\"SELECT * FROM {input_table_name}\"\n",
    "    print(f\"Executing query: {query}\")\n",
    "    \n",
    "    # table_env.execute_sql(query).print()\n",
    "    \n",
    "\n",
    "\n",
    "# Entry point for the script\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5e26107-4838-4024-b962-aba6753412e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_env.execute_sql(\"show tables \" ).print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73f4325b-5d04-400e-843f-d3d48300f730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_env.execute_sql(\"select * from  input_table \" ).print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "289d90d1-51dd-4115-adc2-006ecfc88713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyflink.common.configuration.Configuration at 0x11793e310>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add required jar files\n",
    "\n",
    "CURRENT_DIR = os.getcwd()\n",
    "\n",
    "# Define a list of JAR file names you want to add\n",
    "jar_files = [\n",
    "    \"flink-s3-fs-hadoop-1.16.1.jar\",\n",
    "    \"hudi-flink1.13-bundle-0.13.1.jar\",\n",
    "    \"flink-sql-connector-kinesis-1.16.1.jar\"\n",
    "]\n",
    "\n",
    "# Build the list of JAR URLs by prepending 'file:///' to each file name\n",
    "jar_urls = [f\"file:///{CURRENT_DIR}/{jar_file}\" for jar_file in jar_files]\n",
    "      \n",
    "table_env.get_config().get_configuration().set_string(\n",
    "    \"pipeline.jars\",\n",
    "    \";\".join(jar_urls)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49727577-237e-4b17-82aa-56f144f502d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DROP TABLE IF EXISTS hand_data;\n",
      "\n",
      "CREATE TABLE hand_data(\n",
      "    event_uuid VARCHAR,\n",
      "    event_name VARCHAR,\n",
      "    event_data VARCHAR,\n",
      "    event_type VARCHAR,\n",
      "    event_timestamp INTEGER,\n",
      "    shard_id INTEGER,\n",
      "    event_source VARCHAR,\n",
      "    event_version VARCHAR\n",
      ")\n",
      "PARTITIONED BY (`event_name`)\n",
      "WITH (\n",
      "    'connector' = 'hudi',\n",
      "    'path' = 's3a://south-test-1/tmp1' ,\n",
      "    'table.type' = 'MERGE_ON_READ' ,\n",
      "    'read.streaming.enabled' = 'true'\n",
      ");\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o332.executeSql.\n: org.apache.flink.table.api.ValidationException: Unable to create a sink for writing table 'default_catalog.default_database.hand_data'.\n\nTable options are:\n\n'connector'='hudi'\n'path'='s3a://south-test-1/tmp1'\n'read.streaming.enabled'='true'\n'table.type'='MERGE_ON_READ'\n\tat org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:331)\n\tat org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:451)\n\tat org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:227)\n\tat org.apache.flink.table.planner.delegation.PlannerBase.$anonfun$translate$1(PlannerBase.scala:177)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)\n\tat scala.collection.Iterator.foreach(Iterator.scala:937)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:937)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1425)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:70)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:69)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:233)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:226)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:177)\n\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1296)\n\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:874)\n\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1112)\n\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:735)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)\n\tat org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.hudi.exception.HoodieIOException: Get table config error\n\tat org.apache.hudi.util.StreamerUtil.getTableConfig(StreamerUtil.java:299)\n\tat org.apache.hudi.table.HoodieTableFactory.setupTableOptions(HoodieTableFactory.java:104)\n\tat org.apache.hudi.table.HoodieTableFactory.createDynamicTableSink(HoodieTableFactory.java:93)\n\tat org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:328)\n\t... 29 more\nCaused by: java.nio.file.AccessDeniedException: s3a://south-test-1/tmp1/.hoodie: getFileStatus on s3a://south-test-1/tmp1/.hoodie: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 7KM7ZE33ESGE7H9B; S3 Extended Request ID: H9shpbgMzSVOEdgAXr06E6x9UZBIcKjdDBX0zjMhufoHIG6sNzBI23Zcg1JeUzac29AGntmQln0=; Proxy: null), S3 Extended Request ID: H9shpbgMzSVOEdgAXr06E6x9UZBIcKjdDBX0zjMhufoHIG6sNzBI23Zcg1JeUzac29AGntmQln0=:403 Forbidden\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:255)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:175)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3796)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$exists$34(S3AFileSystem.java:4703)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4701)\n\tat org.apache.hudi.util.StreamerUtil.getTableConfig(StreamerUtil.java:295)\n\t... 32 more\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 7KM7ZE33ESGE7H9B; S3 Extended Request ID: H9shpbgMzSVOEdgAXr06E6x9UZBIcKjdDBX0zjMhufoHIG6sNzBI23Zcg1JeUzac29AGntmQln0=; Proxy: null), S3 Extended Request ID: H9shpbgMzSVOEdgAXr06E6x9UZBIcKjdDBX0zjMhufoHIG6sNzBI23Zcg1JeUzac29AGntmQln0=\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1819)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1403)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1372)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5259)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5206)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1360)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$10(S3AFileSystem.java:2545)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:414)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:377)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2533)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2513)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3776)\n\t... 40 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m         table_env\u001b[38;5;241m.\u001b[39mexecute_sql(statement\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Define the data to be inserted into the Hudi table\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[43mtable_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;43m    INSERT INTO hand_data\u001b[39;49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;43m        SELECT * FROM input_table\u001b[39;49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mprint()\n",
      "File \u001b[0;32m~/Documents/Spark/venv/lib/python3.9/site-packages/pyflink/table/table_environment.py:837\u001b[0m, in \u001b[0;36mTableEnvironment.execute_sql\u001b[0;34m(self, stmt)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;124;03mExecute the given single statement, and return the execution result.\u001b[39;00m\n\u001b[1;32m    825\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.11.0\u001b[39;00m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_before_execute()\n\u001b[0;32m--> 837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m TableResult(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_j_tenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecuteSql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstmt\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/Spark/venv/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/Spark/venv/lib/python3.9/site-packages/pyflink/util/exceptions.py:146\u001b[0m, in \u001b[0;36mcapture_java_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyflink\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjava_gateway\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_gateway\n",
      "File \u001b[0;32m~/Documents/Spark/venv/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o332.executeSql.\n: org.apache.flink.table.api.ValidationException: Unable to create a sink for writing table 'default_catalog.default_database.hand_data'.\n\nTable options are:\n\n'connector'='hudi'\n'path'='s3a://south-test-1/tmp1'\n'read.streaming.enabled'='true'\n'table.type'='MERGE_ON_READ'\n\tat org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:331)\n\tat org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:451)\n\tat org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:227)\n\tat org.apache.flink.table.planner.delegation.PlannerBase.$anonfun$translate$1(PlannerBase.scala:177)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)\n\tat scala.collection.Iterator.foreach(Iterator.scala:937)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:937)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1425)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:70)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:69)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:233)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:226)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:177)\n\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1296)\n\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:874)\n\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1112)\n\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:735)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)\n\tat org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.hudi.exception.HoodieIOException: Get table config error\n\tat org.apache.hudi.util.StreamerUtil.getTableConfig(StreamerUtil.java:299)\n\tat org.apache.hudi.table.HoodieTableFactory.setupTableOptions(HoodieTableFactory.java:104)\n\tat org.apache.hudi.table.HoodieTableFactory.createDynamicTableSink(HoodieTableFactory.java:93)\n\tat org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:328)\n\t... 29 more\nCaused by: java.nio.file.AccessDeniedException: s3a://south-test-1/tmp1/.hoodie: getFileStatus on s3a://south-test-1/tmp1/.hoodie: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 7KM7ZE33ESGE7H9B; S3 Extended Request ID: H9shpbgMzSVOEdgAXr06E6x9UZBIcKjdDBX0zjMhufoHIG6sNzBI23Zcg1JeUzac29AGntmQln0=; Proxy: null), S3 Extended Request ID: H9shpbgMzSVOEdgAXr06E6x9UZBIcKjdDBX0zjMhufoHIG6sNzBI23Zcg1JeUzac29AGntmQln0=:403 Forbidden\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:255)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:175)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3796)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$exists$34(S3AFileSystem.java:4703)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4701)\n\tat org.apache.hudi.util.StreamerUtil.getTableConfig(StreamerUtil.java:295)\n\t... 32 more\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 7KM7ZE33ESGE7H9B; S3 Extended Request ID: H9shpbgMzSVOEdgAXr06E6x9UZBIcKjdDBX0zjMhufoHIG6sNzBI23Zcg1JeUzac29AGntmQln0=; Proxy: null), S3 Extended Request ID: H9shpbgMzSVOEdgAXr06E6x9UZBIcKjdDBX0zjMhufoHIG6sNzBI23Zcg1JeUzac29AGntmQln0=\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1819)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1403)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1372)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5259)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5206)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1360)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$10(S3AFileSystem.java:2545)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:414)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:377)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2533)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2513)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3776)\n\t... 40 more\n"
     ]
    }
   ],
   "source": [
    "# write to hudi \n",
    "s3_bucket = 'south-test-1'\n",
    "hudi_output_path = f's3a://{s3_bucket}/tmp1'\n",
    "\n",
    "hudi_sink = f\"\"\"\n",
    "DROP TABLE IF EXISTS hand_data;\n",
    "\n",
    "CREATE TABLE hand_data(\n",
    "    event_uuid VARCHAR,\n",
    "    event_name VARCHAR,\n",
    "    event_data VARCHAR,\n",
    "    event_type VARCHAR,\n",
    "    event_timestamp INTEGER,\n",
    "    shard_id INTEGER,\n",
    "    event_source VARCHAR,\n",
    "    event_version VARCHAR\n",
    ")\n",
    "PARTITIONED BY (`event_name`)\n",
    "WITH (\n",
    "    'connector' = 'hudi',\n",
    "    'path' = '{hudi_output_path}' ,\n",
    "    'table.type' = 'MERGE_ON_READ' ,\n",
    "    'read.streaming.enabled' = 'true'\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "print(f\"{hudi_sink}\")\n",
    "\n",
    "# Execute the SQL to create the Hudi table\n",
    "# table_env.execute_sql(hudi_sink)\n",
    "\n",
    "\n",
    "# Execute the DDL statements\n",
    "for statement in hudi_sink.strip().split(';'):\n",
    "    if statement.strip():\n",
    "        table_env.execute_sql(statement.strip())\n",
    "\n",
    "# Define the data to be inserted into the Hudi table\n",
    "table_env.execute_sql(\"\"\"\n",
    "    INSERT INTO hand_data\n",
    "        SELECT * FROM input_table\n",
    "\"\"\").print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "230023d3-5f66-40d3-a89e-75823866888e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|  table name |\n",
      "+-------------+\n",
      "|   hand_data |\n",
      "| input_table |\n",
      "+-------------+\n",
      "2 rows in set\n"
     ]
    }
   ],
   "source": [
    "table_env.execute_sql(\"\"\"\n",
    "    show tables\n",
    "\"\"\").print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6ffa82e-2382-4f4f-b79c-146d00c55ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "table_env.execute_sql(\"\"\"\n",
    "    drop table input_table\n",
    "\"\"\").print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4116dc-383b-40df-8ba1-34fdb6b4d13f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
